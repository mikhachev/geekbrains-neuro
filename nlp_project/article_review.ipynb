{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проекта выбрал тему NLP, с задачами полезными для текущей работы. Есть журнал трудов Крыловского научного центра https://transactions-ksrc.ru/ \n",
    "из области судостроения, авторы присылают свои научные статьи. Есть задачи по определению в какую категорию отнести статью, а также определение ключевых слов по присланной статье в формате docx. Задача с ключевыми словами более сложная, поэтому для начала полезно почитать научные статьи, заодно это будет полезно и для задачи классификации, включая методы предварительной обработки текста.  \n",
    "На тему нейронных сетей для этих  есть уже немало статей, однако на практике гораздо больше вижу без них.\n",
    "Выбрал статью: \n",
    "\n",
    "## Using Convolutional Neural Networks to Extract Keywords \n",
    "## and Keyphrases About Foodborne Illnesses\n",
    "https://atrium.lib.uoguelph.ca/xmlui/bitstream/handle/10214/15923/Wang_Jingjing_201904_Msc.pdf?sequence=1&isAllowed=y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ней сравниваются различные алгоритмы и неожиданно применяется CNN, которая обычно применяется не для текста, а для распознавания изображений.\n",
    "Исследователи в области глубокого обучения обычно применяют  Рекуррентные Нейронные Сети (RNN) для выполнения задач извлечения ключевых слов или фраз. Однако время выполнения для RNN обычно занимает много времени.  Существует растущая тенденция применять CNN к задачам НЛП из-за его способности выполнять извлечение функций и быстрое выполнение. Кроме того, сделано  исследование различных методов обучения с учителем и  обучения без учителя, чтобы выполнить извлечение ключевых слов и фраз с двумя различными типами наборов данных.\n",
    "Как неконтролируемый подход  был примен LDA (Latent Dirichlet Allocation). Для полуконтролируемых подходов внедрен Guided-LDA, а как контролируемый подход использован KEA (Key Extraction Algorithm) и CNN.\n",
    "Обзор методов извлечения ключевых слов и ключевых фраз\n",
    "RAKE (быстрое автоматическое извлечение ключевых слов) было предложено для задач поиска информации. Исследователи определили ключевые слова-кандидаты, используя список ключевых слов, и рассчитали количество слов на основе степени и частоты значения совпадений слов на графике. Ключевые слова или фразы были выбраны на основе ранжированных оценок.\n",
    "Более поздние методы начали комбинировать статистические методы с технологиями НЛП для определения ключевых слов или ключевых фраз. Kathait et al. (2017) сначала применили фильтры стоп-слов и частей речи для извлечения ключевых слов-кандидатов, затем применили лингвистические и статистические характеристики, используя значения вхождения и совпадения слова, чтобы взвесить ключевые слова-кандидаты. Наконец, процесс ранжирования был применен, и наиболее эффективные фразы были рассмотрены в качестве ключевых фраз. Другой подход рассматривает представление слова для автоматического извлечения Ключевые слова. Он применил word2vec для представления слов и вычислил близость между словами. График был нарисован, чтобы прервать отношения между словами, чтобы идентифицировать ключевые слова.\n",
    "\n",
    "Для формирования датасета была выполнена загрузка большого количества научных статей с веб-сайта PubMed с использованием первоначальных ключевых слов и ключевых фраз, специально посвященных вспышке болезней пищевого происхождения. По этой тематике не существует полного набора ключевых слов и фраз, поэтому для их составления помогал эксперт в данной области.\n",
    "Однако недостатком метода контролируемого обучения является то, что он зависит от большого помеченного набора данных. Большинство этих аннотированных данные были созданы экспертами по предметной области, что отнимает много времени. \n",
    "Эксперт помог  аннотировать начальный набор данных.  Собранный набор данных:  4988 документов по теме вспышек болезней пищевого происхождения. Кроме того, использован TF-IDF для интеллектуальной интерпретации большего количества данных. Окончательный набор аннотированных ключевых слов и ключевых фраз с 2290 ключевыми словами, в том числе 705 униграмм и 1585 биграмм. После сбора данных предварительно обработаны эти наборы данных и сопоставлены их с окончательным набором аннотированных ключевых слов и ключевых фраз. Кроме того, мы создано два разных типа наборов данных: весь набор данных и классифицированные наборы данных.\n",
    "\n",
    "Предварительная обработка данных\n",
    "1.\tУдаление статей, написанные на других языках, кроме английского\n",
    "2.\tКонвертация статьи PDF в формат TXT\n",
    "3.\tОчистка текста и шумов для каждого документа. Выделено  несколько типов шумов, которые требуют внимания: ссылки, URL-адреса, цифры, специальные знаки препинания и разделенные слова. Эти шумы были устранены скриптом  Java:  генератор ом лексического анализатора JFlex. Для LDA все знаки препинания были удалены на этапе предварительной обработки данных с помощью пакета Python Gensim. Некоторые слова были разделены разрывом строки. Это увеличило количество бессмысленных слов, одновременно с потерей важных слов. Для решения этой проблемы были применены регулярные выражения для объединения разделенных слов.\n",
    "4.\tТокенизация используется для разбиения абзацев на сегменты для обработки их в алгоритмы. Токенизация предложений разбивает документы на отдельные предложения, а токенизация слов разбивает документы на отдельные слова. Токенизация слов была применена к LDA, GuidedLDA и KEA. Токенизация предложений была применена к CNN.\n",
    "5.\tУдаление стоп-слов. В исследовании использовались английские стоп-слова из пакетов Python nltk13, sklearn и spacy, а также вручную добавленный список английских стоп-слов от исследователей.\n",
    "6.\tlowercase()\n",
    "7.\tУдалить короткие слова (менее 2 символов)\n",
    "Методы обучения без учителя машинного обучения для  задач извлечения ключевых слов и фраз можно рассматривать как задачи ранжирования. MIKE (извлечение ключевой фразы путем интеграции многомерной информации) - это алгоритм случайного блуждания на основе графа. Подход содержал пять шагов. Во-первых, слова-кандидаты были идентифицированы с помощью фильтров стоп-слов и частей речи (POS). Во-вторых, был составлен граф слов, основанный на совместном появлении слов-кандидатов из документов. В-третьих, в параметрическую модель случайного блуждания были интегрированы различные типы информации, такие как особенности, совпадения и распределения тем. В-четвертых, параметрическая модель автоматически определяет вес объектов (узел и ребро). Наконец, ключевые фразы были определены путем ранжирования суммы баллов отдельных ключевых слов.\n",
    "Рекуррентные нейронные сети (RNN) широко используются для задач НЛП в различных областях. Была предложена архитектура Deep RNN путем добавления объединенного уровня для определения ключевых фраз по данным Twitter. Исследователи использовали вывод первого слоя, чтобы выбрать ключевые слова-кандидаты (True или False), а затем применили второй слой для определения положения ключевых фраз (Single, Begin, Middle, End и Not). Один конкретный тип RNN, сеть с кратковременной кратковременной памятью (LSTM), использовался в отрасли здравоохранения для выделения ключевых фраз. В исследование Мейна и соавт. применили модель кодека RNN для генерации ключевых фраз. Механизм копирования был добавлен к этой модели для обработки редких фраз в научных статьях. Механизм копирования является решением, которое компенсирует отсутствие RNN, поскольку RNN не квалифицирован для обработки ключевых фраз, содержащих слова, которых нет в словаре.\n",
    "Сверточные нейронные сети для текста\n",
    "Из-за возможностей извлечения функций и быстрого времени выполнения растет тенденция использования CNN для задач НЛП. Фактически, входные данные задач НЛП представляют собой либо предложения, либо документы, которые разбиты токенами на этапе предварительной обработки данных. Каждый токен обозначает каждое слово; чтобы обработать эти слова на компьютере, токены переводятся в числовой формат, который похож на представление изображений. Ввод задач НЛП состоит из слов-векторов; вектор слова считается входным «изображением» для CNN.\n",
    "В целях машиночитаемости входные слова должны быть преобразованы в представление числовых векторов. Традиционным способом решения этой задачи является использование одного горячего кодирования, которое преобразует входные слова в векторы двоичных значений: 0 и 1. Каждое слово представляется в виде списка: его длина является общим числом слова в корпусе. Это выбранное слово помечается как 1, а другие позиции заполняются 0. Однако недостатком одной горячей кодировки является то, что она не выражает лингвистическое сходство и / или контекстную информацию между словами. Кроме того, одно горячее кодирование приведет к высокой размерности и разреженному пространству. Чтобы решить эти проблемы, команда Google  создала новый алгоритм встраивания слов: word2vec. Основная цель этого алгоритма - создать эффективное представление слов в векторном пространстве с семантической, контекстной и иерархической информацией. Этот подход предсказывает набор слов из контекстных слов, используя двухслойную структуру нейронных сетей. Поскольку word2vec извлекает контекстные элементы из окружающего содержимого, аналогичные слова будут занимать местоположения, которые являются проксимальными в векторном пространстве, тогда как слова, которые несут совершенно разные значения, будут расположены еще дальше.\n",
    "Сверточный слой применяется для извлечения объектов. Изучаемый объект скользит по входному объему и создает карту объектов или карту активации. Функция активации выпрямленных линейных единиц (ReLU) применяется к сверточному уровню.\n",
    "В целях уменьшения общего объема вычислений, а также предотвращения подгонки для модели применяется слой пула. Объединение в пул использует два различных метода: максимальное объединение и среднее объединение. Чтобы найти максимальных вкладчиков в классификатор, эта работа использует Max-pooling. CNN присоединяет полностью связанный уровень к концу своей архитектуры. Полностью связанный слой можно рассматривать как традиционную нейронную сеть, которая принимает входной объем и выдает выходной результат в качестве результата прогнозирования. Объем ввода - это выход из слоя пула. В этом слое применяется функция активации сигмоида.\n",
    "Гиперпараметры для этого эксперимента включают 11 частей:\n",
    "1.\tWord embedding. Вложение слов встраивает каждое слово в одномерный вектор. Размерность вложения слова относится к размеру этого одномерного вектора.\n",
    "2.\tМинимальное количество вхождений слова.\n",
    "3.\tРазмеры ядра также являются размером фильтра. Что касается задач НЛП, размеры ядра представляют скользящее действие в n-граммах. Если размер ядра равен 1, алгоритм скользит через униграммы; если размер ядра равен 2, алгоритм скользит по биграммам. \n",
    "4.\tКоличество фильтров представляет размер объекта\n",
    "5.\tDropout Rate \n",
    "6.\tbatch size. Длина последовательности указывает длину предложений, которые проходят через модель CNN. Для подачи предложений в модель CNN важно иметь одинаковую длину для каждого предложения. Если реальная длина предложения меньше длины последовательности, она будет заполнена на 0 для остальной части позиции.\n",
    "7.\tStrides - количество шагов, которое фильтры смещают по входу.\n",
    "8.\tРазмер пула представляет размер пула\n",
    "9.\tOptimizer.  Адам оптимизация является одним из самых полезных методов оптимизации; это быстро и дает хороший результат для алгоритма глубокого обучения\n",
    "10.\tЭпохи. Эпоха жертвует один прямой проход и один обратный проход расчета\n",
    "11.\tРанняя остановка - это тип регуляризации для предотвращения переоснащения\n",
    "TF-IDF\n",
    "TF-IDF объединяет два веса слова в документе из коллекции. Один вес обозначает частоту терминов (TF). Второй вес - это частота обратных документов (IDF). TF-IDF широко используется для задач НЛП, таких как категоризация текста, поиск информации и выбор текстовых функций, извлечение ключевых слов, также используется для удаления стоп-слов, поскольку стоп-слова, как правило, предоставляют менее полезную информацию.\n",
    "Скрытое распределение Дирихле (LDA) является генеративно-вероятностной моделью. У DA есть три элемента: слова, документы и темы. Каждый документ представляет собой набор тем, и каждая тема содержит набор слов. LDA - это тематическая модель, которая способствует извлечению ключевых слов, поскольку предоставляет лучшие слова, представляющие тему в документах. Для данного корпуса он генерирует распределения слов и разделов по темам. LDA также является моделью «bag of words», она преобразует документы в пакеты слов без какого-либо порядка между ними.\n",
    "У LDA есть три гиперпараметра: α, β и количество тем.  α управляет распределением тем для документов, а β представляет распределение слов по темам. Высокое значение α означает, что значения тем отличаются друг от друга, наоборот, низкое значение α означает, что\n",
    "Смысл тем близок друг к другу. Высокое значение β указывает на то, что в темах больше разборчивых слов. Напротив, низкий β описывает, что темы, вероятно, содержат меньше уникальных слов. Количество тем - это заданное значение модели, которое всегда присваивается пользователями.\n",
    "КЕА\n",
    "Алгоритм извлечения ключей (KEA) – контролируемый алгоритм, который применяет Naive Bayes model с особенностями значения слова TF-IDF и значения первого вхождения для вычисления вероятности ключевых фраз-кандидатов. КЕА имеет два этапа: этап обучения и этап извлечения. Этап обучения используется для построения модели и этап извлечения используют модель для определения ключевых слов или ключевых фраз. Две функции рассчитываются на двух этапах: TF-IDF и первое появление\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Глава 3\n",
    "В разделе предложенного подхода представлен новый алгоритм извлечения ключевых слов и фраз с использованием CNN. Растет тенденция к использованию CNN, а не RNN для задач НЛП, благодаря его способности выполнять более быстрое время обучения и достигать более точного семантического понимания. CNN сократил время выполнения в 9 раз с точки зрения языкового перевода. способность к быстрому выполнению обучения и пониманию содержания вдохновила нас на разработку нового решения: применять CNN для определения ключевых слов и ключевых фраз, а также сравнивать различные неконтролируемые, полуобслуживаемые / контролируемые методы машинного обучения для извлечения ключевых слов и ключевых фраз при болезнях пищевого происхождения. ,\n",
    "Предлагаемый подход\n",
    "Модель CNN содержит входной уровень, сверточный уровень, объединяющий слой и полностью связанный слой. Входной слой принимает последовательность из m слов для предложения, каждое из которых представлено k-мерным вектором, что приводит к матрице m*k. В сверточном слое модель использует несколько фильтров 2 размеров: 1 и 2 строки (представляющих соответственно униграммы и биграммы), где размер каждого фильтра должен совпадать с размером входного сигнала, поскольку каждая строка представляет отдельное слово или функцию. CNN использует фильтры размера 1 и 2 вместо {3,4,5}, которые обычно используются в архитектурах CNN во многих задачах. Такой выбор дизайна оправдан необходимостью захвата униграмм и биграмм, поскольку оценка ограничена отдельными словами и парами слов. Выходной слой выполняет двоичную классификацию того, содержит ли данное предложение важные слова или фразы. На этапе оценки вычисленные результаты сверяются с аннотированными метками. Если предложение содержит ключевое слово или ключевую фразу, оно будет помечено как «ДА»; в противном случае он будет помечен как «НЕТ».\n",
    "Шаг 1: Определите наиболее подходящее ключевое слово (униграмма) и ключевую фразу (биграмма) в каждом предложении из каждой карты объектов.\n",
    "Шаг 2. Присвойте веса ключевым словам и фразам на основе их частот.\n",
    "После обучения модели CNN следующая задача - определить ключевые слова и ключевые фразы. Для этого мы прослеживаем путь назад от результата классификации «ДА» до уровня сцепления с максимальным пулом, а затем до входных токенов. Это определяет, какой узел в слое max-pooling вносит наибольший вклад в задачу классификации и соответствует его соответствующему значению из входных данных.\n",
    "Эксперт по изучаемой предметной области  предоставил первоначальный аннотированный набор ключевых слов и ключевых фраз (всего 383), а также 23 категории, основываясь на своих знаниях, после просмотра сотен статей, связанных с областью болезней пищевого происхождения. Из-за ограниченного аннотированного набора данных в этом исследовании применялся метод обучения без учителя (TFIDF), чтобы автоматически аннотировать больше ключевых слов и ключевых фраз. Этот метод был выбран, из-за отсутствия адекватных аннотированных ключевых слов и фраз для выполнения обучения под надзором для этой задачи. Кроме того, TF-IDF прост в реализации и эффективен для подготовки расширенного набора аннотированных ключевых слов и ключевых фраз.\n",
    "Первый эксперимент проверяет эффективность метода обучения без учителя: LDA. Во втором эксперименте рассматривается эффективность методов обучения с учителем: GuidedLDA, KEA и CNN. В этом эксперименте униграммы и биграммы извлекаются одновременно с использованием CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Глава 4 Эксперименты и результаты\n",
    "В обучающем наборе данных было 3874 файла, а в наборе данных тестирования - 1114 файлов.\n",
    "\n",
    "Эксперимент 2: обучение с учителем\n",
    "Чтобы разбить документы на предложения применили простую утилиту https://spacy.io/ для работы с токенами,. удалены те предложения, длина которых меньше или равна 3 после удаления стоп-слова. В этом случае эти предложения несут менее значимую информацию, поскольку они могут быть получены из таблиц или рисунков в оригинальной PDF-версии статей.\n",
    "В этом эксперименте подготовены три различных набора выходов CNN:\n",
    "• CNN-униграммы: только аннотированные униграммы были применены для построения выходных данных. Если предложение\n",
    "содержал по крайней мере одну униграмму, вывод для этого предложения был «ДА».\n",
    "• Биграммы CNN: только аннотированные биграммы были применены для построения выходных данных. Если предложение\n",
    "содержал по крайней мере один биграмм, вывод для этого предложения был «ДА».\n",
    "• CNN-mix: аннотированные и биграммы были применены для создания выходных данных. Если предложение\n",
    "содержал хотя бы одну униграмму или хотя бы одну биграмму, вывод для этого предложения был\n",
    "\"ДА\".\n",
    "\n",
    "Построение  модели CNN\n",
    "В этом проекте входные данные - это одно измерение. Форма имеет значение (sequence_length,), где sequence_length равно 310, как обсуждалось в разделе 2.3, 0 были заполнены в позиции для этих длин предложений, которые меньше 310.\n",
    "Embedding Layer: мы применили word2vec в gensim.models, чтобы узнать веса для каждого слова. В этом эксперименте input_dim равнялся размеру словаря,\n",
    "output_dim был 300, а input_length равнялся 310.\n",
    "Dropout Layer: выпадающий слой принимает коэффициент отсева\n",
    "Слой Convolutuon1D для фильтра размера 1: функция активации Relu была применена к каждому слою Convolutuon1D.\n",
    "Слой MaxPooling1D для фильтра размера 1: шаг был 1, а размер пула был 2\n",
    "Flatten Layer для фильтра размера 1: тензор выровнялся так, чтобы он имел ту же форму, что и количество элементов в этом тензоре.\n",
    "Слой Convolutuon1D для фильтра размера 2\n",
    "Слой MaxPooling1D для фильтра размера 2\n",
    "Сверточный слой для фильтра размера 2\n",
    "Concatenate Layer:: объединяет значения из каждого слоя MaxPooling1D. Это можно считать входом полностью связанного слоя.\n",
    "Dropout layer\n",
    "Dense Layer: выполнял потерянную функцию, которая является линейной операцией сцепленного слоя после процесса выпадения.\n",
    "Activation Layer (Output Layer): слой активации является выходным слоем. функции активационная функция- cигмоида применена. Этот слой имеет только один выход. Если выходные данные больше или равны 0,5, они попадают в категорию «ДА», поскольку в этом предложении есть хотя бы одно ключевое слово. И наоборот, если результат был меньше 0,5, он был отнесен к категории «НЕТ», что указывает на то, что в этом предложении нет ключевых слов.\n",
    "Размер пакета варьировался от 16 до 128. Размеры ядра - это размеры фильтра, мы выбрали 1 и 2 - униграммы и биграммы соответственно.\n",
    "SHARCNET18 - это передовая исследовательская вычислительная платформа, в которой 18 канадских академических учреждений совместно используют сеть высокопроизводительных компьютеров. В исследовании  воспользовались одной из ее высокопроизводительных и кластерных систем: медная система20 предлагает максимум 128,0 ГБ памяти и 24 узла ЦП с 4 картами K80 (8 устройств GK210) в каждом узле.\n",
    "Показано, что контролируемые методы обучения (CNN и KEA) превзошли как полуконтролируемые (GuidedLDA), так и неконтролируемые методы обучения (LDA) с точки зрения извлечения ключевых слов и фраз.\n",
    "CNN-униграммы показали лучший результат (с оценкой F1 87,73%) для униграммы\n",
    "экстракция CNN-биграммы показали лучшие результаты при экстракции биграммы с показателем F1 61,44% (таблица 4.14).\n",
    "LDA: 40,74% и 25,72%\n",
    "Кроме того, результаты TF-IDF страдали от ограниченного знания предметной области. Исследователи сравнили результаты TD-IDF с KEA. Результаты показывают, что KEA получила лучшую оценку F1 по извлечению ключевых слов и фраз.\n",
    "Прошлые исследования показали, что причина того, что KEA работает хорошо, состоит в том, что, поскольку KEA не только рассчитывает значение TF-IDF, KEA также учитывает первое значение появления\n",
    "Стандартный LDA работает плохо, когда речь идет об областях знаний, таких как биология, поскольку стандартный LDA не учитывает знание предметной области. включение знаний о предметной области в модели LDA привело к высокой производительности в определенных областях с точки зрения задач извлечения ключевых слов. KEA уступил по сравнению с CNN, но был значительно лучше, чем LDA и GuidedLDA. наша работа показывает, что KEA показала превосходную производительность с небольшим набором данных, что также объясняется в оригинальной статье, которая представила KEA. Эта  работа способствует заполнению пробелов, оставленных первичным применением CNN для задач классификации на уровне предложений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
